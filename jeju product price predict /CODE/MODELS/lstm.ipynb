{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../../DATA/train.csv')\n",
    "test  = pd.read_csv('../../DATA/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing + Imputation of Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 타입 변경, 열 이름 변경 \n",
    "\n",
    "new_column_names = {\n",
    "    'corporation': 'corp',\n",
    "    'location': 'loc',\n",
    "    'supply(kg)': 'supply',\n",
    "    'price(원/kg)': 'price',\n",
    "}\n",
    "\n",
    "train = train.rename(columns=new_column_names)\n",
    "test = test.rename(columns=new_column_names)\n",
    "\n",
    "train['timestamp']  = pd.to_datetime(train['timestamp'])\n",
    "test['timestamp']  = pd.to_datetime(test['timestamp'])\n",
    "\n",
    "train['newitem'] = train['item'].str.cat([train['corp'], train['loc']], sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_info(data) :\n",
    "    # data의 날짜 정보 추출하기 \n",
    "    \n",
    "    data['year'] = data['timestamp'].dt.year\n",
    "    data['month'] = data['timestamp'].dt.month\n",
    "    data['day'] = data['timestamp'].dt.day\n",
    "    data['weekday'] = data['timestamp'].dt.weekday\n",
    "    data['quarter'] = data['timestamp'].dt.quarter\n",
    "    data['weekofyear'] = data['timestamp'].dt.isocalendar().week\n",
    "    data['dayofyear'] = data['timestamp'].dt.dayofyear #해당 년도의 몇 일째 \n",
    "    data['holi'] = 0 \n",
    "    data.loc[(data['holi'] == 0) & (data['weekday'] >= 6), 'holi'] = 1\n",
    "    \n",
    "get_date_info(train)\n",
    "get_date_info(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 보간할 날, 가격 데이터 생성 \n",
    "average_price = train.groupby(['year', 'month', 'item', 'corp','loc'])['price'].mean().astype(int).reset_index()\n",
    "\n",
    "\n",
    "emptydays  = ['2019-01-01',  '2019-02-05', '2019-02-06', '2019-09-13', '2019-09-14', '2020-01-01', '2020-01-25', '2020-01-27', '2020-10-01', '2020-10-02', '2020-10-03', '2021-01-01', '2021-02-12', '2021-02-13', \n",
    "'2021-09-20', '2021-09-21', '2021-09-22', '2022-01-01', '2022-01-31', '2022-02-01', '2022-02-02', '2022-09-10', '2022-09-12', '2023-01-23', '2023-01-24']\n",
    "# 25일에 해당하는 값들을 보간해야해 \n",
    "emptydays = [datetime.strptime(day, \"%Y-%m-%d\") for day in emptydays]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공휴일 가격 데이터 보간하기 \n",
    "for i in range(len(train)) :\n",
    "    if train.loc[i, 'timestamp'] in emptydays :\n",
    "        year,month,item, corp, loc = train.loc[i,'year'],train.loc[i,'month'],train.loc[i,'item'],train.loc[i,'corp'],train.loc[i,'loc']\n",
    "        newprice = average_price[(average_price['year']==year) & (average_price['month']==month) & (average_price['item']==item) & (average_price['corp']==corp) & (average_price['loc']==loc)]['price']\n",
    "        train.loc[i,'price'] = newprice.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#가격 시계열 price_data 생성하기 \n",
    "\n",
    "unique_values = train['newitem'].unique()\n",
    "# 날짜 범위 생성\n",
    "start_date = datetime(2019, 1, 1).date()\n",
    "end_date =datetime(2023, 3, 3).date()\n",
    "dates = pd.date_range(start_date, end_date, freq='D')\n",
    "\n",
    "# 데이터프레임 생성\n",
    "price_data = pd.DataFrame(columns=['item'] + dates.strftime('%Y-%m-%d').tolist())\n",
    "price_data['item'] = unique_values\n",
    "\n",
    "# print(len(price_data)) 39 \n",
    "for i in range(len(price_data)) :\n",
    "    price_data.iloc[i,1:] = train['price'][i*1523 : (i+1)*1523 ]\n",
    "price_data['item'] = price_data['item'].astype(str)\n",
    "price_data['corp'] = price_data['item'].str[3:4]\n",
    "\n",
    "price_data['loca'] = price_data['item'].str[-1:]\n",
    "price_data['product'] = price_data['item'].str[:2]\n",
    "price_data['product_loca'] =price_data['product'].str.cat(price_data['loca'], sep=' ')\n",
    "cols = price_data.columns[-4:]  # 가장 뒤의 3개 열의 열 이름을 선택\n",
    "price_data = price_data[cols.tolist() + price_data.columns[:-4].tolist()]  # 열 순서 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiwoongyoon/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/kiwoongyoon/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "price_data['product_loca'] = le.fit_transform(price_data['product_loca'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corp</th>\n",
       "      <th>loca</th>\n",
       "      <th>product</th>\n",
       "      <th>product_loca</th>\n",
       "      <th>item</th>\n",
       "      <th>2019-01-01</th>\n",
       "      <th>2019-01-02</th>\n",
       "      <th>2019-01-03</th>\n",
       "      <th>2019-01-04</th>\n",
       "      <th>2019-01-05</th>\n",
       "      <th>...</th>\n",
       "      <th>2023-02-22</th>\n",
       "      <th>2023-02-23</th>\n",
       "      <th>2023-02-24</th>\n",
       "      <th>2023-02-25</th>\n",
       "      <th>2023-02-26</th>\n",
       "      <th>2023-02-27</th>\n",
       "      <th>2023-02-28</th>\n",
       "      <th>2023-03-01</th>\n",
       "      <th>2023-03-02</th>\n",
       "      <th>2023-03-03</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>J</td>\n",
       "      <td>TG</td>\n",
       "      <td>8</td>\n",
       "      <td>TG A J</td>\n",
       "      <td>1513.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1728.0</td>\n",
       "      <td>1408.0</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2813.0</td>\n",
       "      <td>2770.0</td>\n",
       "      <td>2633.0</td>\n",
       "      <td>3155.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2657.0</td>\n",
       "      <td>3922.0</td>\n",
       "      <td>3397.0</td>\n",
       "      <td>3195.0</td>\n",
       "      <td>3640.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>S</td>\n",
       "      <td>TG</td>\n",
       "      <td>9</td>\n",
       "      <td>TG A S</td>\n",
       "      <td>1859.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2526.0</td>\n",
       "      <td>2134.0</td>\n",
       "      <td>2075.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3444.0</td>\n",
       "      <td>3481.0</td>\n",
       "      <td>3518.0</td>\n",
       "      <td>4201.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4166.0</td>\n",
       "      <td>4009.0</td>\n",
       "      <td>4173.0</td>\n",
       "      <td>4219.0</td>\n",
       "      <td>4089.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>J</td>\n",
       "      <td>TG</td>\n",
       "      <td>8</td>\n",
       "      <td>TG B J</td>\n",
       "      <td>1231.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1692.0</td>\n",
       "      <td>1516.0</td>\n",
       "      <td>1471.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4175.0</td>\n",
       "      <td>6216.0</td>\n",
       "      <td>3558.0</td>\n",
       "      <td>2412.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3540.0</td>\n",
       "      <td>3141.0</td>\n",
       "      <td>6382.0</td>\n",
       "      <td>3558.0</td>\n",
       "      <td>3470.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>S</td>\n",
       "      <td>TG</td>\n",
       "      <td>9</td>\n",
       "      <td>TG B S</td>\n",
       "      <td>1512.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1944.0</td>\n",
       "      <td>1815.0</td>\n",
       "      <td>1717.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3202.0</td>\n",
       "      <td>3478.0</td>\n",
       "      <td>3939.0</td>\n",
       "      <td>3677.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4057.0</td>\n",
       "      <td>3821.0</td>\n",
       "      <td>4037.0</td>\n",
       "      <td>4004.0</td>\n",
       "      <td>4241.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C</td>\n",
       "      <td>J</td>\n",
       "      <td>TG</td>\n",
       "      <td>8</td>\n",
       "      <td>TG C J</td>\n",
       "      <td>1649.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>1794.0</td>\n",
       "      <td>1773.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4017.0</td>\n",
       "      <td>4585.0</td>\n",
       "      <td>4835.0</td>\n",
       "      <td>5550.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5037.0</td>\n",
       "      <td>2643.0</td>\n",
       "      <td>3742.0</td>\n",
       "      <td>3983.0</td>\n",
       "      <td>5175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C</td>\n",
       "      <td>S</td>\n",
       "      <td>TG</td>\n",
       "      <td>9</td>\n",
       "      <td>TG C S</td>\n",
       "      <td>1517.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2078.0</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>1815.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3228.0</td>\n",
       "      <td>3484.0</td>\n",
       "      <td>3773.0</td>\n",
       "      <td>4298.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4180.0</td>\n",
       "      <td>4234.0</td>\n",
       "      <td>4357.0</td>\n",
       "      <td>4466.0</td>\n",
       "      <td>4748.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>D</td>\n",
       "      <td>J</td>\n",
       "      <td>TG</td>\n",
       "      <td>8</td>\n",
       "      <td>TG D J</td>\n",
       "      <td>1164.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1616.0</td>\n",
       "      <td>1337.0</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1838.0</td>\n",
       "      <td>1829.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>D</td>\n",
       "      <td>S</td>\n",
       "      <td>TG</td>\n",
       "      <td>9</td>\n",
       "      <td>TG D S</td>\n",
       "      <td>1652.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>1757.0</td>\n",
       "      <td>1719.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2539.0</td>\n",
       "      <td>2955.0</td>\n",
       "      <td>3323.0</td>\n",
       "      <td>3321.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3409.0</td>\n",
       "      <td>3236.0</td>\n",
       "      <td>4631.0</td>\n",
       "      <td>4114.0</td>\n",
       "      <td>4146.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>E</td>\n",
       "      <td>J</td>\n",
       "      <td>TG</td>\n",
       "      <td>8</td>\n",
       "      <td>TG E J</td>\n",
       "      <td>1167.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1692.0</td>\n",
       "      <td>1475.0</td>\n",
       "      <td>1392.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3157.0</td>\n",
       "      <td>4793.0</td>\n",
       "      <td>2403.0</td>\n",
       "      <td>1489.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2620.0</td>\n",
       "      <td>1673.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1904.0</td>\n",
       "      <td>1622.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>E</td>\n",
       "      <td>S</td>\n",
       "      <td>TG</td>\n",
       "      <td>9</td>\n",
       "      <td>TG E S</td>\n",
       "      <td>1584.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2151.0</td>\n",
       "      <td>1829.0</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3400.0</td>\n",
       "      <td>3349.0</td>\n",
       "      <td>3026.0</td>\n",
       "      <td>2946.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3418.0</td>\n",
       "      <td>3141.0</td>\n",
       "      <td>4235.0</td>\n",
       "      <td>3960.0</td>\n",
       "      <td>3791.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>A</td>\n",
       "      <td>J</td>\n",
       "      <td>CR</td>\n",
       "      <td>4</td>\n",
       "      <td>CR A J</td>\n",
       "      <td>637.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1141.0</td>\n",
       "      <td>1133.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2377.0</td>\n",
       "      <td>2236.0</td>\n",
       "      <td>2224.0</td>\n",
       "      <td>2136.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1934.0</td>\n",
       "      <td>2162.0</td>\n",
       "      <td>2101.0</td>\n",
       "      <td>1839.0</td>\n",
       "      <td>3486.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>B</td>\n",
       "      <td>J</td>\n",
       "      <td>CR</td>\n",
       "      <td>4</td>\n",
       "      <td>CR B J</td>\n",
       "      <td>321.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1177.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>2274.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>C</td>\n",
       "      <td>J</td>\n",
       "      <td>CR</td>\n",
       "      <td>4</td>\n",
       "      <td>CR C J</td>\n",
       "      <td>756.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1280.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1399.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2192.0</td>\n",
       "      <td>2339.0</td>\n",
       "      <td>2120.0</td>\n",
       "      <td>2116.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>2060.0</td>\n",
       "      <td>1890.0</td>\n",
       "      <td>1788.0</td>\n",
       "      <td>1758.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>D</td>\n",
       "      <td>J</td>\n",
       "      <td>CR</td>\n",
       "      <td>4</td>\n",
       "      <td>CR D J</td>\n",
       "      <td>867.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>1135.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2256.0</td>\n",
       "      <td>2238.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2212.0</td>\n",
       "      <td>2050.0</td>\n",
       "      <td>2051.0</td>\n",
       "      <td>1933.0</td>\n",
       "      <td>1725.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>D</td>\n",
       "      <td>S</td>\n",
       "      <td>CR</td>\n",
       "      <td>5</td>\n",
       "      <td>CR D S</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>E</td>\n",
       "      <td>J</td>\n",
       "      <td>CR</td>\n",
       "      <td>4</td>\n",
       "      <td>CR E J</td>\n",
       "      <td>892.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1163.0</td>\n",
       "      <td>1128.0</td>\n",
       "      <td>1176.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2029.0</td>\n",
       "      <td>2209.0</td>\n",
       "      <td>2097.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2111.0</td>\n",
       "      <td>2152.0</td>\n",
       "      <td>2085.0</td>\n",
       "      <td>1931.0</td>\n",
       "      <td>1777.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>E</td>\n",
       "      <td>S</td>\n",
       "      <td>CR</td>\n",
       "      <td>5</td>\n",
       "      <td>CR E S</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A</td>\n",
       "      <td>J</td>\n",
       "      <td>CB</td>\n",
       "      <td>2</td>\n",
       "      <td>CB A J</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>374.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>1625.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1438.0</td>\n",
       "      <td>710.0</td>\n",
       "      <td>488.0</td>\n",
       "      <td>1312.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>A</td>\n",
       "      <td>S</td>\n",
       "      <td>CB</td>\n",
       "      <td>3</td>\n",
       "      <td>CB A S</td>\n",
       "      <td>177.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>510.0</td>\n",
       "      <td>534.0</td>\n",
       "      <td>511.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>680.0</td>\n",
       "      <td>821.0</td>\n",
       "      <td>848.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>D</td>\n",
       "      <td>J</td>\n",
       "      <td>CB</td>\n",
       "      <td>2</td>\n",
       "      <td>CB D J</td>\n",
       "      <td>207.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>827.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>748.0</td>\n",
       "      <td>809.0</td>\n",
       "      <td>798.0</td>\n",
       "      <td>858.0</td>\n",
       "      <td>780.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>E</td>\n",
       "      <td>J</td>\n",
       "      <td>CB</td>\n",
       "      <td>2</td>\n",
       "      <td>CB E J</td>\n",
       "      <td>169.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>A</td>\n",
       "      <td>J</td>\n",
       "      <td>RD</td>\n",
       "      <td>6</td>\n",
       "      <td>RD A J</td>\n",
       "      <td>277.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>367.0</td>\n",
       "      <td>460.0</td>\n",
       "      <td>402.0</td>\n",
       "      <td>...</td>\n",
       "      <td>492.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>633.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>486.0</td>\n",
       "      <td>548.0</td>\n",
       "      <td>552.0</td>\n",
       "      <td>608.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>A</td>\n",
       "      <td>S</td>\n",
       "      <td>RD</td>\n",
       "      <td>7</td>\n",
       "      <td>RD A S</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>414.0</td>\n",
       "      <td>499.0</td>\n",
       "      <td>431.0</td>\n",
       "      <td>...</td>\n",
       "      <td>534.0</td>\n",
       "      <td>381.0</td>\n",
       "      <td>455.0</td>\n",
       "      <td>545.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>495.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>498.0</td>\n",
       "      <td>596.0</td>\n",
       "      <td>613.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>C</td>\n",
       "      <td>S</td>\n",
       "      <td>RD</td>\n",
       "      <td>7</td>\n",
       "      <td>RD C S</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>D</td>\n",
       "      <td>J</td>\n",
       "      <td>RD</td>\n",
       "      <td>6</td>\n",
       "      <td>RD D J</td>\n",
       "      <td>291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>382.0</td>\n",
       "      <td>431.0</td>\n",
       "      <td>394.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>537.0</td>\n",
       "      <td>642.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>D</td>\n",
       "      <td>S</td>\n",
       "      <td>RD</td>\n",
       "      <td>7</td>\n",
       "      <td>RD D S</td>\n",
       "      <td>281.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>396.0</td>\n",
       "      <td>445.0</td>\n",
       "      <td>408.0</td>\n",
       "      <td>...</td>\n",
       "      <td>479.0</td>\n",
       "      <td>528.0</td>\n",
       "      <td>495.0</td>\n",
       "      <td>573.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>626.0</td>\n",
       "      <td>571.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>609.0</td>\n",
       "      <td>601.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>E</td>\n",
       "      <td>J</td>\n",
       "      <td>RD</td>\n",
       "      <td>6</td>\n",
       "      <td>RD E J</td>\n",
       "      <td>287.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>470.0</td>\n",
       "      <td>450.0</td>\n",
       "      <td>...</td>\n",
       "      <td>593.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>557.0</td>\n",
       "      <td>664.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>561.0</td>\n",
       "      <td>615.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>616.0</td>\n",
       "      <td>666.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>E</td>\n",
       "      <td>S</td>\n",
       "      <td>RD</td>\n",
       "      <td>7</td>\n",
       "      <td>RD E S</td>\n",
       "      <td>216.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>437.0</td>\n",
       "      <td>492.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>555.0</td>\n",
       "      <td>616.0</td>\n",
       "      <td>709.0</td>\n",
       "      <td>678.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>A</td>\n",
       "      <td>J</td>\n",
       "      <td>BC</td>\n",
       "      <td>0</td>\n",
       "      <td>BC A J</td>\n",
       "      <td>1673.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2559.0</td>\n",
       "      <td>2425.0</td>\n",
       "      <td>2097.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2611.0</td>\n",
       "      <td>2071.0</td>\n",
       "      <td>2028.0</td>\n",
       "      <td>2222.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2357.0</td>\n",
       "      <td>3307.0</td>\n",
       "      <td>3392.0</td>\n",
       "      <td>2936.0</td>\n",
       "      <td>2850.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>A</td>\n",
       "      <td>S</td>\n",
       "      <td>BC</td>\n",
       "      <td>1</td>\n",
       "      <td>BC A S</td>\n",
       "      <td>1770.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2188.0</td>\n",
       "      <td>2455.0</td>\n",
       "      <td>2242.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2378.0</td>\n",
       "      <td>2197.0</td>\n",
       "      <td>2060.0</td>\n",
       "      <td>2222.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2378.0</td>\n",
       "      <td>3218.0</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>3067.0</td>\n",
       "      <td>2875.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>B</td>\n",
       "      <td>J</td>\n",
       "      <td>BC</td>\n",
       "      <td>0</td>\n",
       "      <td>BC B J</td>\n",
       "      <td>1636.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2440.0</td>\n",
       "      <td>2348.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2160.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>2097.0</td>\n",
       "      <td>2299.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2610.0</td>\n",
       "      <td>3035.0</td>\n",
       "      <td>3323.0</td>\n",
       "      <td>2685.0</td>\n",
       "      <td>2558.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>B</td>\n",
       "      <td>S</td>\n",
       "      <td>BC</td>\n",
       "      <td>1</td>\n",
       "      <td>BC B S</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>C</td>\n",
       "      <td>J</td>\n",
       "      <td>BC</td>\n",
       "      <td>0</td>\n",
       "      <td>BC C J</td>\n",
       "      <td>1597.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2079.0</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>1849.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2289.0</td>\n",
       "      <td>2051.0</td>\n",
       "      <td>1864.0</td>\n",
       "      <td>2069.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1859.0</td>\n",
       "      <td>2873.0</td>\n",
       "      <td>2839.0</td>\n",
       "      <td>2070.0</td>\n",
       "      <td>2934.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>C</td>\n",
       "      <td>S</td>\n",
       "      <td>BC</td>\n",
       "      <td>1</td>\n",
       "      <td>BC C S</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1625.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>D</td>\n",
       "      <td>J</td>\n",
       "      <td>BC</td>\n",
       "      <td>0</td>\n",
       "      <td>BC D J</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2253.0</td>\n",
       "      <td>2175.0</td>\n",
       "      <td>1981.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1938.0</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>2355.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2473.0</td>\n",
       "      <td>2896.0</td>\n",
       "      <td>3487.0</td>\n",
       "      <td>3185.0</td>\n",
       "      <td>3059.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>E</td>\n",
       "      <td>J</td>\n",
       "      <td>BC</td>\n",
       "      <td>0</td>\n",
       "      <td>BC E J</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2379.0</td>\n",
       "      <td>2257.0</td>\n",
       "      <td>1857.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2112.0</td>\n",
       "      <td>1903.0</td>\n",
       "      <td>1912.0</td>\n",
       "      <td>2234.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2492.0</td>\n",
       "      <td>3480.0</td>\n",
       "      <td>3793.0</td>\n",
       "      <td>3144.0</td>\n",
       "      <td>3045.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>E</td>\n",
       "      <td>S</td>\n",
       "      <td>BC</td>\n",
       "      <td>1</td>\n",
       "      <td>BC E S</td>\n",
       "      <td>1551.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2301.0</td>\n",
       "      <td>1880.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2365.0</td>\n",
       "      <td>2195.0</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>2353.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2488.0</td>\n",
       "      <td>3232.0</td>\n",
       "      <td>3816.0</td>\n",
       "      <td>3321.0</td>\n",
       "      <td>2939.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>CB</td>\n",
       "      <td>2</td>\n",
       "      <td>CB F J</td>\n",
       "      <td>331.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>478.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>536.0</td>\n",
       "      <td>...</td>\n",
       "      <td>671.0</td>\n",
       "      <td>631.0</td>\n",
       "      <td>641.0</td>\n",
       "      <td>579.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>652.0</td>\n",
       "      <td>672.0</td>\n",
       "      <td>621.0</td>\n",
       "      <td>653.0</td>\n",
       "      <td>643.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>RD</td>\n",
       "      <td>6</td>\n",
       "      <td>RD F J</td>\n",
       "      <td>272.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>395.0</td>\n",
       "      <td>437.0</td>\n",
       "      <td>425.0</td>\n",
       "      <td>...</td>\n",
       "      <td>440.0</td>\n",
       "      <td>396.0</td>\n",
       "      <td>388.0</td>\n",
       "      <td>429.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>574.0</td>\n",
       "      <td>523.0</td>\n",
       "      <td>529.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39 rows × 1528 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   corp loca product  product_loca    item 2019-01-01 2019-01-02 2019-01-03  \\\n",
       "0     A    J      TG             8  TG A J     1513.0        0.0     1728.0   \n",
       "1     A    S      TG             9  TG A S     1859.0        0.0     2526.0   \n",
       "2     B    J      TG             8  TG B J     1231.0        0.0     1692.0   \n",
       "3     B    S      TG             9  TG B S     1512.0        0.0     1944.0   \n",
       "4     C    J      TG             8  TG C J     1649.0        0.0     1965.0   \n",
       "5     C    S      TG             9  TG C S     1517.0        0.0     2078.0   \n",
       "6     D    J      TG             8  TG D J     1164.0        0.0     1616.0   \n",
       "7     D    S      TG             9  TG D S     1652.0        0.0     2048.0   \n",
       "8     E    J      TG             8  TG E J     1167.0        0.0     1692.0   \n",
       "9     E    S      TG             9  TG E S     1584.0        0.0     2151.0   \n",
       "10    A    J      CR             4  CR A J      637.0        0.0        0.0   \n",
       "11    B    J      CR             4  CR B J      321.0        0.0     1177.0   \n",
       "12    C    J      CR             4  CR C J      756.0        0.0     1280.0   \n",
       "13    D    J      CR             4  CR D J      867.0        0.0        0.0   \n",
       "14    D    S      CR             5  CR D S      128.0        0.0        0.0   \n",
       "15    E    J      CR             4  CR E J      892.0        0.0     1163.0   \n",
       "16    E    S      CR             5  CR E S       35.0        0.0        0.0   \n",
       "17    A    J      CB             2  CB A J      111.0        0.0        0.0   \n",
       "18    A    S      CB             3  CB A S      177.0        0.0      510.0   \n",
       "19    D    J      CB             2  CB D J      207.0        0.0        0.0   \n",
       "20    E    J      CB             2  CB E J      169.0        0.0        0.0   \n",
       "21    A    J      RD             6  RD A J      277.0        0.0      367.0   \n",
       "22    A    S      RD             7  RD A S      286.0        0.0      414.0   \n",
       "23    C    S      RD             7  RD C S        0.0        0.0        0.0   \n",
       "24    D    J      RD             6  RD D J      291.0        0.0      382.0   \n",
       "25    D    S      RD             7  RD D S      281.0        0.0      396.0   \n",
       "26    E    J      RD             6  RD E J      287.0        0.0        0.0   \n",
       "27    E    S      RD             7  RD E S      216.0        0.0        0.0   \n",
       "28    A    J      BC             0  BC A J     1673.0        0.0     2559.0   \n",
       "29    A    S      BC             1  BC A S     1770.0        0.0     2188.0   \n",
       "30    B    J      BC             0  BC B J     1636.0        0.0     2440.0   \n",
       "31    B    S      BC             1  BC B S        0.0        0.0        0.0   \n",
       "32    C    J      BC             0  BC C J     1597.0        0.0     2079.0   \n",
       "33    C    S      BC             1  BC C S       52.0        0.0        0.0   \n",
       "34    D    J      BC             0  BC D J     1620.0        0.0     2253.0   \n",
       "35    E    J      BC             0  BC E J     1600.0        0.0     2379.0   \n",
       "36    E    S      BC             1  BC E S     1551.0        0.0        0.0   \n",
       "37    F    J      CB             2  CB F J      331.0        0.0      478.0   \n",
       "38    F    J      RD             6  RD F J      272.0        0.0      395.0   \n",
       "\n",
       "   2019-01-04 2019-01-05  ... 2023-02-22 2023-02-23 2023-02-24 2023-02-25  \\\n",
       "0      1408.0     1250.0  ...     2813.0     2770.0     2633.0     3155.0   \n",
       "1      2134.0     2075.0  ...     3444.0     3481.0     3518.0     4201.0   \n",
       "2      1516.0     1471.0  ...     4175.0     6216.0     3558.0     2412.0   \n",
       "3      1815.0     1717.0  ...     3202.0     3478.0     3939.0     3677.0   \n",
       "4      1794.0     1773.0  ...     4017.0     4585.0     4835.0     5550.0   \n",
       "5      2002.0     1815.0  ...     3228.0     3484.0     3773.0     4298.0   \n",
       "6      1337.0     1234.0  ...        0.0        0.0     1838.0     1829.0   \n",
       "7      1757.0     1719.0  ...     2539.0     2955.0     3323.0     3321.0   \n",
       "8      1475.0     1392.0  ...     3157.0     4793.0     2403.0     1489.0   \n",
       "9      1829.0     2002.0  ...     3400.0     3349.0     3026.0     2946.0   \n",
       "10     1141.0     1133.0  ...     2377.0     2236.0     2224.0     2136.0   \n",
       "11        0.0        0.0  ...        0.0        0.0     3000.0        0.0   \n",
       "12        0.0     1399.0  ...     2192.0     2339.0     2120.0     2116.0   \n",
       "13     1326.0     1135.0  ...     2256.0     2238.0     2018.0        0.0   \n",
       "14        0.0        0.0  ...        0.0        0.0        0.0        0.0   \n",
       "15     1128.0     1176.0  ...     2029.0     2209.0     2097.0        0.0   \n",
       "16        0.0        0.0  ...        0.0        0.0        0.0        0.0   \n",
       "17        0.0      374.0  ...     1562.0     1562.0     1625.0        0.0   \n",
       "18      534.0      511.0  ...        0.0        0.0        0.0        0.0   \n",
       "19        0.0        0.0  ...        0.0        0.0        0.0      827.0   \n",
       "20        0.0        0.0  ...        0.0        0.0        0.0        0.0   \n",
       "21      460.0      402.0  ...      492.0      260.0      633.0        0.0   \n",
       "22      499.0      431.0  ...      534.0      381.0      455.0      545.0   \n",
       "23        0.0        0.0  ...        0.0        0.0        0.0        0.0   \n",
       "24      431.0      394.0  ...        0.0        0.0        0.0        0.0   \n",
       "25      445.0      408.0  ...      479.0      528.0      495.0      573.0   \n",
       "26      470.0      450.0  ...      593.0        0.0      557.0      664.0   \n",
       "27      437.0      492.0  ...        0.0        0.0        0.0        0.0   \n",
       "28     2425.0     2097.0  ...     2611.0     2071.0     2028.0     2222.0   \n",
       "29     2455.0     2242.0  ...     2378.0     2197.0     2060.0     2222.0   \n",
       "30     2348.0     2100.0  ...     2160.0     2014.0     2097.0     2299.0   \n",
       "31        0.0        0.0  ...        0.0        0.0        0.0        0.0   \n",
       "32     2020.0     1849.0  ...     2289.0     2051.0     1864.0     2069.0   \n",
       "33        0.0     1625.0  ...        0.0        0.0        0.0        0.0   \n",
       "34     2175.0     1981.0  ...     1938.0     2006.0     1956.0     2355.0   \n",
       "35     2257.0     1857.0  ...     2112.0     1903.0     1912.0     2234.0   \n",
       "36     2301.0     1880.0  ...     2365.0     2195.0     2025.0     2353.0   \n",
       "37      600.0      536.0  ...      671.0      631.0      641.0      579.0   \n",
       "38      437.0      425.0  ...      440.0      396.0      388.0      429.0   \n",
       "\n",
       "   2023-02-26 2023-02-27 2023-02-28 2023-03-01 2023-03-02 2023-03-03  \n",
       "0         0.0     2657.0     3922.0     3397.0     3195.0     3640.0  \n",
       "1         0.0     4166.0     4009.0     4173.0     4219.0     4089.0  \n",
       "2         0.0     3540.0     3141.0     6382.0     3558.0     3470.0  \n",
       "3         0.0     4057.0     3821.0     4037.0     4004.0     4241.0  \n",
       "4         0.0     5037.0     2643.0     3742.0     3983.0     5175.0  \n",
       "5         0.0     4180.0     4234.0     4357.0     4466.0     4748.0  \n",
       "6         0.0        0.0        0.0        0.0        0.0        0.0  \n",
       "7         0.0     3409.0     3236.0     4631.0     4114.0     4146.0  \n",
       "8         0.0     2620.0     1673.0        0.0     1904.0     1622.0  \n",
       "9         0.0     3418.0     3141.0     4235.0     3960.0     3791.0  \n",
       "10        0.0     1934.0     2162.0     2101.0     1839.0     3486.0  \n",
       "11        0.0     3000.0     2274.0        0.0        0.0        0.0  \n",
       "12        0.0     2002.0     2060.0     1890.0     1788.0     1758.0  \n",
       "13        0.0     2212.0     2050.0     2051.0     1933.0     1725.0  \n",
       "14        0.0        0.0        0.0        0.0        0.0        0.0  \n",
       "15        0.0     2111.0     2152.0     2085.0     1931.0     1777.0  \n",
       "16        0.0        0.0        0.0        0.0        0.0        0.0  \n",
       "17        0.0     1438.0      710.0      488.0     1312.0        0.0  \n",
       "18        0.0        0.0        0.0      680.0      821.0      848.0  \n",
       "19        0.0      748.0      809.0      798.0      858.0      780.0  \n",
       "20        0.0        0.0        0.0        0.0        0.0        0.0  \n",
       "21        0.0     1900.0      486.0      548.0      552.0      608.0  \n",
       "22        0.0      495.0      462.0      498.0      596.0      613.0  \n",
       "23        0.0        0.0        0.0        0.0        0.0        0.0  \n",
       "24        0.0        0.0        0.0        0.0      537.0      642.0  \n",
       "25        0.0      626.0      571.0      600.0      609.0      601.0  \n",
       "26        0.0      561.0      615.0        0.0      616.0      666.0  \n",
       "27        0.0      555.0      616.0      709.0      678.0        0.0  \n",
       "28        0.0     2357.0     3307.0     3392.0     2936.0     2850.0  \n",
       "29        0.0     2378.0     3218.0     3838.0     3067.0     2875.0  \n",
       "30        0.0     2610.0     3035.0     3323.0     2685.0     2558.0  \n",
       "31        0.0        0.0        0.0        0.0        0.0        0.0  \n",
       "32        0.0     1859.0     2873.0     2839.0     2070.0     2934.0  \n",
       "33        0.0        0.0        0.0        0.0        0.0        0.0  \n",
       "34        0.0     2473.0     2896.0     3487.0     3185.0     3059.0  \n",
       "35        0.0     2492.0     3480.0     3793.0     3144.0     3045.0  \n",
       "36        0.0     2488.0     3232.0     3816.0     3321.0     2939.0  \n",
       "37        0.0      652.0      672.0      621.0      653.0      643.0  \n",
       "38        0.0      468.0      531.0      574.0      523.0      529.0  \n",
       "\n",
       "[39 rows x 1528 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before DeepLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "    def __getitem__(self, index):\n",
    "        if self.Y is not None:\n",
    "            return torch.Tensor(self.X[index]), torch.Tensor(self.Y[index])\n",
    "        return torch.Tensor(self.X[index])\n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    best_loss = 9999999\n",
    "    best_model = None\n",
    "    for epoch in range(1, 9):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        train_mae = []\n",
    "        for X, Y in tqdm(iter(train_loader)):\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "        print(f'Epoch : [{epoch}] Train Loss : [{np.mean(train_loss):.5f}] Val Loss : []')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, Y in tqdm(iter(val_loader)):\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "    return np.mean(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, test_loader, device):\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X in tqdm(iter(test_loader)):\n",
    "            X = X.to(device)\n",
    "            output = model(X)\n",
    "            output = output.cpu().numpy()\n",
    "            predictions.extend(output)\n",
    "\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[43, 42, 9402283]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_list=[43,42,9402283]\n",
    "seed_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf00054c00243c6a10aa0fcadd5a105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Window Size:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49f9998ad768471290088988e496e03c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Learning Rate:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dcfc569550d426b8881909dd1c1dc99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch Size:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7413a4b78e4fee810809cea48a9615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Seed:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/kiwoongyoon/바탕화면/DACON/JEJU PRODUCT PRICE PREDICT/CODE/MODELS/deeplearning1.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/kiwoongyoon/%EB%B0%94%ED%83%95%ED%99%94%EB%A9%B4/DACON/JEJU%20PRODUCT%20PRICE%20PREDICT/CODE/MODELS/deeplearning1.ipynb#X24sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m model \u001b[39m=\u001b[39m BaseModel()\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/kiwoongyoon/%EB%B0%94%ED%83%95%ED%99%94%EB%A9%B4/DACON/JEJU%20PRODUCT%20PRICE%20PREDICT/CODE/MODELS/deeplearning1.ipynb#X24sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(params \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mparameters(), lr \u001b[39m=\u001b[39m CFG[\u001b[39m\"\u001b[39m\u001b[39mLEARNING_RATE\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/kiwoongyoon/%EB%B0%94%ED%83%95%ED%99%94%EB%A9%B4/DACON/JEJU%20PRODUCT%20PRICE%20PREDICT/CODE/MODELS/deeplearning1.ipynb#X24sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m infer_model , fin_loss  \u001b[39m=\u001b[39m train(model, optimizer, train_loader, device)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/kiwoongyoon/%EB%B0%94%ED%83%95%ED%99%94%EB%A9%B4/DACON/JEJU%20PRODUCT%20PRICE%20PREDICT/CODE/MODELS/deeplearning1.ipynb#X24sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m \u001b[39mprint\u001b[39m(fin_loss ,l_rate, batchisze,trainwindowsize)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/kiwoongyoon/%EB%B0%94%ED%83%95%ED%99%94%EB%A9%B4/DACON/JEJU%20PRODUCT%20PRICE%20PREDICT/CODE/MODELS/deeplearning1.ipynb#X24sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m test_dataset \u001b[39m=\u001b[39m CustomDataset(test_input, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[1;32m/home/kiwoongyoon/바탕화면/DACON/JEJU PRODUCT PRICE PREDICT/CODE/MODELS/deeplearning1.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/kiwoongyoon/%EB%B0%94%ED%83%95%ED%99%94%EB%A9%B4/DACON/JEJU%20PRODUCT%20PRICE%20PREDICT/CODE/MODELS/deeplearning1.ipynb#X24sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(output, Y)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/kiwoongyoon/%EB%B0%94%ED%83%95%ED%99%94%EB%A9%B4/DACON/JEJU%20PRODUCT%20PRICE%20PREDICT/CODE/MODELS/deeplearning1.ipynb#X24sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/kiwoongyoon/%EB%B0%94%ED%83%95%ED%99%94%EB%A9%B4/DACON/JEJU%20PRODUCT%20PRICE%20PREDICT/CODE/MODELS/deeplearning1.ipynb#X24sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/kiwoongyoon/%EB%B0%94%ED%83%95%ED%99%94%EB%A9%B4/DACON/JEJU%20PRODUCT%20PRICE%20PREDICT/CODE/MODELS/deeplearning1.ipynb#X24sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m     train_loss\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/kiwoongyoon/%EB%B0%94%ED%83%95%ED%99%94%EB%A9%B4/DACON/JEJU%20PRODUCT%20PRICE%20PREDICT/CODE/MODELS/deeplearning1.ipynb#X24sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m==\u001b[39m CFG[\u001b[39m'\u001b[39m\u001b[39mEPOCHS\u001b[39m\u001b[39m'\u001b[39m] :\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    232\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 234\u001b[0m     adam(params_with_grad,\n\u001b[1;32m    235\u001b[0m          grads,\n\u001b[1;32m    236\u001b[0m          exp_avgs,\n\u001b[1;32m    237\u001b[0m          exp_avg_sqs,\n\u001b[1;32m    238\u001b[0m          max_exp_avg_sqs,\n\u001b[1;32m    239\u001b[0m          state_steps,\n\u001b[1;32m    240\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    241\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    242\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    243\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    244\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    245\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    246\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    247\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    248\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    249\u001b[0m          differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    250\u001b[0m          fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    251\u001b[0m          grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    252\u001b[0m          found_inf\u001b[39m=\u001b[39;49mfound_inf)\n\u001b[1;32m    254\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 300\u001b[0m func(params,\n\u001b[1;32m    301\u001b[0m      grads,\n\u001b[1;32m    302\u001b[0m      exp_avgs,\n\u001b[1;32m    303\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    304\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    305\u001b[0m      state_steps,\n\u001b[1;32m    306\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    307\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    308\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    309\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    310\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    311\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    312\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    313\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    314\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    315\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    316\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:410\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    408\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    409\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 410\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39;49m bias_correction2_sqrt)\u001b[39m.\u001b[39;49madd_(eps)\n\u001b[1;32m    412\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# price data로 학습시켜야 한다\n",
    "seed_list = [42,43,92891012]\n",
    "for trainwindowsize in tqdm([28, 35, 42, 49], desc=\"Train Window Size\"):\n",
    "    for l_rate in tqdm([0.01, 0.02, 0.001, 0.0015, 0.002], desc=\"Learning Rate\"):\n",
    "        for batchisze in tqdm([32, 64, 128, 256, 512], desc=\"Batch Size\"):\n",
    "            for seed in tqdm([42], desc=\"Seed\"):\n",
    "               \n",
    "                CFG = {\n",
    "                'TRAIN_WINDOW_SIZE':trainwindowsize,\n",
    "                'PREDICT_SIZE':28,\n",
    "                'EPOCHS':10,\n",
    "                'LEARNING_RATE':l_rate,\n",
    "                'BATCH_SIZE':batchisze,\n",
    "                'SEED':seed\n",
    "                }\n",
    "\n",
    "                seed_everything(CFG['SEED'])\n",
    "                for shop in (range(1)):\n",
    "                    train_data= price_data\n",
    "                    numeric_cols = train_data.columns[5:]\n",
    "                    def make_train_data(data, train_size=CFG['TRAIN_WINDOW_SIZE'], predict_size=CFG['PREDICT_SIZE']):\n",
    "                        num_rows = len(data)\n",
    "                       \n",
    "                        # windowsize = 35 +28  = 63\n",
    "                       \n",
    "                        window_size = train_size + predict_size\n",
    "                        input_data = np.empty((num_rows * (len(data.columns) - window_size + 1-5), train_size, 1))\n",
    "                        target_data = np.empty((num_rows * (len(data.columns) - window_size + 1-5), predict_size))\n",
    "\n",
    "                        for i in (range(num_rows)):\n",
    "                            sales_data = np.array(data.iloc[i, 5:])  # 첫 4개 열을 제외하고 가져옵니다.\n",
    "                           \n",
    "                            for j in range(len(sales_data) - window_size + 1):\n",
    "                                window = sales_data[j : j + window_size]\n",
    "                                input_data[i * (len(data.columns) - window_size + 1-5) + j] = window[:train_size].reshape(-1, 1)\n",
    "                                target_data[i * (len(data.columns) - window_size + 1-5) + j] = window[train_size:]\n",
    "\n",
    "                       \n",
    "                        return input_data, target_data\n",
    "\n",
    "                    def make_predict_data(data, train_size=CFG['TRAIN_WINDOW_SIZE']):\n",
    "\n",
    "                        num_rows = len(data)\n",
    "\n",
    "                        input_data = np.empty((num_rows, train_size, 1))\n",
    "\n",
    "                        for i in (range(num_rows)):\n",
    "                            sales_data = np.array(data.iloc[i, -train_size:])\n",
    "                            input_data[i] = sales_data.reshape(-1, 1)  \n",
    "\n",
    "                        return input_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    # print(f\"seed is {seed}\")\n",
    "                    train_input, train_target = make_train_data(train_data)\n",
    "                    test_input = make_predict_data(train_data)\n",
    "                    data_len = len(train_input)\n",
    "\n",
    "                    train_dataset = CustomDataset(train_input, train_target)\n",
    "                    train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
    "                   \n",
    "                    class BaseModel(nn.Module):\n",
    "                        def __init__(self, input_size=1, hidden_size=512, output_size=CFG['PREDICT_SIZE']):\n",
    "                            super(BaseModel, self).__init__()\n",
    "                            self.hidden_size = hidden_size\n",
    "                            self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "                            self.fc = nn.Sequential(\n",
    "                                nn.Linear(hidden_size*2, hidden_size//2),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Dropout(),\n",
    "                                nn.Linear(hidden_size//2, output_size)\n",
    "                            )\n",
    "\n",
    "                            self.actv = nn.ReLU()\n",
    "\n",
    "                        def forward(self, x):\n",
    "                            batch_size = x.size(0)\n",
    "                            hidden = self.init_hidden(batch_size, x.device)\n",
    "                            lstm_out, hidden = self.lstm(x, hidden)\n",
    "                            last_output = lstm_out[:, -1, :]\n",
    "                            output = self.actv(self.fc(last_output))\n",
    "                            return output.squeeze(1)\n",
    "\n",
    "                        def init_hidden(self, batch_size, device):\n",
    "                            return (torch.zeros(2, batch_size, self.hidden_size, device=device),\n",
    "                                    torch.zeros(2, batch_size, self.hidden_size, device=device))\n",
    "\n",
    "                    def train(model, optimizer, train_loader, device):\n",
    "                        model.to(device)\n",
    "                        criterion = nn.MSELoss().to(device)\n",
    "                        best_loss = 9999999\n",
    "                        best_model = None\n",
    "                        last_loss = 0\n",
    "\n",
    "                        for epoch in range(1, CFG['EPOCHS']+1):\n",
    "                            model.train()\n",
    "                            train_loss = []\n",
    "                            train_mae = []\n",
    "                            for X, Y in (iter(train_loader)):\n",
    "                                X = X.to(device)\n",
    "                                Y = Y.to(device)\n",
    "                                optimizer.zero_grad()\n",
    "                                output = model(X)\n",
    "                                loss = criterion(output, Y)\n",
    "                                loss.backward()\n",
    "                                optimizer.step()\n",
    "\n",
    "                                train_loss.append(loss.item())\n",
    "                               \n",
    "                            if epoch == CFG['EPOCHS'] :\n",
    "                                last_loss = np.mean(train_loss)\n",
    "                            # print(f'Epoch : [{epoch}] Train Loss : [{np.mean(train_loss):.5f}] Val Loss : []')\n",
    "                        return model , last_loss\n",
    "                   \n",
    "                    model = BaseModel()\n",
    "                    optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "                    infer_model , fin_loss  = train(model, optimizer, train_loader, device)\n",
    "                    print(fin_loss ,l_rate, batchisze,trainwindowsize)\n",
    "                    test_dataset = CustomDataset(test_input, None)\n",
    "                    test_loader = DataLoader(test_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "                    pred = inference(infer_model, test_loader, device)\n",
    "                   \n",
    "                    # print(pred.shape)\n",
    "                    # print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1701193.5035430838 0.002 128 49\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f6438b5f7e44d2a9489750d015ed8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# price data로 학습시켜야 한다 \n",
    "# 1701193.5035430838 0.002 128 49\n",
    "seed_list = [42,43,92891012]\n",
    "\n",
    "for trainwindowsize in [49]:\n",
    "    \n",
    "    for l_rate in [0.002]:\n",
    "        \n",
    "        for batchisze in [128] :\n",
    "             \n",
    "            \n",
    "\n",
    "            for seed in [42]:\n",
    "                \n",
    "                CFG = {\n",
    "                'TRAIN_WINDOW_SIZE':trainwindowsize, \n",
    "                'PREDICT_SIZE':28, \n",
    "                'EPOCHS':10,\n",
    "                'LEARNING_RATE':l_rate,\n",
    "                'BATCH_SIZE':batchisze,\n",
    "                'SEED':seed\n",
    "                }\n",
    "\n",
    "                seed_everything(CFG['SEED'])\n",
    "                for shop in (range(1)):\n",
    "                    train_data= price_data\n",
    "                    numeric_cols = train_data.columns[5:]\n",
    "                    def make_train_data(data, train_size=CFG['TRAIN_WINDOW_SIZE'], predict_size=CFG['PREDICT_SIZE']):\n",
    "                        num_rows = len(data)\n",
    "                        \n",
    "                        # windowsize = 35 +28  = 63 \n",
    "                        \n",
    "                        window_size = train_size + predict_size\n",
    "                        input_data = np.empty((num_rows * (len(data.columns) - window_size + 1-5), train_size, 1))\n",
    "                        target_data = np.empty((num_rows * (len(data.columns) - window_size + 1-5), predict_size))\n",
    "\n",
    "                        for i in (range(num_rows)):\n",
    "                            sales_data = np.array(data.iloc[i, 5:])  # 첫 4개 열을 제외하고 가져옵니다.\n",
    "                            \n",
    "                            for j in range(len(sales_data) - window_size + 1):\n",
    "                                window = sales_data[j : j + window_size]\n",
    "                                input_data[i * (len(data.columns) - window_size + 1-5) + j] = window[:train_size].reshape(-1, 1)\n",
    "                                target_data[i * (len(data.columns) - window_size + 1-5) + j] = window[train_size:]\n",
    "\n",
    "                        \n",
    "                        return input_data, target_data\n",
    "\n",
    "                    def make_predict_data(data, train_size=CFG['TRAIN_WINDOW_SIZE']):\n",
    "\n",
    "                        num_rows = len(data)\n",
    "\n",
    "                        input_data = np.empty((num_rows, train_size, 1))\n",
    "\n",
    "                        for i in (range(num_rows)):\n",
    "                            sales_data = np.array(data.iloc[i, -train_size:])\n",
    "                            input_data[i] = sales_data.reshape(-1, 1)  \n",
    "\n",
    "                        return input_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    # print(f\"seed is {seed}\")\n",
    "                    train_input, train_target = make_train_data(train_data)\n",
    "                    test_input = make_predict_data(train_data)\n",
    "                    data_len = len(train_input)\n",
    "\n",
    "                    train_dataset = CustomDataset(train_input, train_target)\n",
    "                    train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
    "                    \n",
    "                    class BaseModel(nn.Module):\n",
    "                        def __init__(self, input_size=1, hidden_size=512, output_size=CFG['PREDICT_SIZE']):\n",
    "                            super(BaseModel, self).__init__()\n",
    "                            self.hidden_size = hidden_size\n",
    "                            self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "                            self.fc = nn.Sequential(\n",
    "                                nn.Linear(hidden_size*2, hidden_size//2),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Dropout(),\n",
    "                                nn.Linear(hidden_size//2, output_size)\n",
    "                            )\n",
    "\n",
    "                            self.actv = nn.ReLU()\n",
    "\n",
    "                        def forward(self, x):\n",
    "                            batch_size = x.size(0)\n",
    "                            hidden = self.init_hidden(batch_size, x.device)\n",
    "                            lstm_out, hidden = self.lstm(x, hidden)\n",
    "                            last_output = lstm_out[:, -1, :]\n",
    "                            output = self.actv(self.fc(last_output))\n",
    "                            return output.squeeze(1)\n",
    "\n",
    "                        def init_hidden(self, batch_size, device):\n",
    "                            return (torch.zeros(2, batch_size, self.hidden_size, device=device),\n",
    "                                    torch.zeros(2, batch_size, self.hidden_size, device=device))\n",
    "\n",
    "                    def train(model, optimizer, train_loader, device):\n",
    "                        model.to(device)\n",
    "                        criterion = nn.MSELoss().to(device)\n",
    "                        best_loss = 9999999\n",
    "                        best_model = None\n",
    "                        last_loss = 0 \n",
    "\n",
    "                        for epoch in range(1, CFG['EPOCHS']+1):\n",
    "                            model.train()\n",
    "                            train_loss = []\n",
    "                            train_mae = []\n",
    "                            for X, Y in (iter(train_loader)):\n",
    "                                X = X.to(device)\n",
    "                                Y = Y.to(device)\n",
    "                                optimizer.zero_grad()\n",
    "                                output = model(X)\n",
    "                                loss = criterion(output, Y)\n",
    "                                loss.backward()\n",
    "                                optimizer.step()\n",
    "\n",
    "                                train_loss.append(loss.item())\n",
    "                                \n",
    "                            if epoch == CFG['EPOCHS'] :\n",
    "                                last_loss = np.mean(train_loss)\n",
    "                            # print(f'Epoch : [{epoch}] Train Loss : [{np.mean(train_loss):.5f}] Val Loss : []')\n",
    "                        return model , last_loss\n",
    "                    \n",
    "                    model = BaseModel()\n",
    "                    optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "                    infer_model , fin_loss  = train(model, optimizer, train_loader, device)\n",
    "                    print(fin_loss ,l_rate, batchisze,trainwindowsize)\n",
    "                    test_dataset = CustomDataset(test_input, None)\n",
    "                    test_loader = DataLoader(test_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "                    pred = inference(infer_model, test_loader, device)\n",
    "                    \n",
    "                    # print(pred.shape)\n",
    "                    # print(pred)\n",
    "\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1092, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pred.shape\n",
    "pred = pred.reshape(-1 , 1)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TG_A_J_20230304</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TG_A_J_20230305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TG_A_J_20230306</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TG_A_J_20230307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TG_A_J_20230308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>RD_F_J_20230327</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>RD_F_J_20230328</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>RD_F_J_20230329</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>RD_F_J_20230330</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>RD_F_J_20230331</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1092 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID  answer\n",
       "0     TG_A_J_20230304       0\n",
       "1     TG_A_J_20230305       0\n",
       "2     TG_A_J_20230306       0\n",
       "3     TG_A_J_20230307       0\n",
       "4     TG_A_J_20230308       0\n",
       "...               ...     ...\n",
       "1087  RD_F_J_20230327       0\n",
       "1088  RD_F_J_20230328       0\n",
       "1089  RD_F_J_20230329       0\n",
       "1090  RD_F_J_20230330       0\n",
       "1091  RD_F_J_20230331       0\n",
       "\n",
       "[1092 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit = pd.read_csv('./../../DATA/sample_submission.csv')\n",
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit['answer'] = pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.loc[submit['answer'] <= 0, 'answer'] = 0\n",
    "submit.loc[submit.index % 7 == 1, 'answer'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('../../DATA/SUBMIT/lstm1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
